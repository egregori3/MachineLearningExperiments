Loading LETTER dataset
Integer Encoding dataset
[[ 2  8  3 ...,  8  0  8]
 [ 5 12  3 ...,  8  4 10]
 [ 4 11  6 ...,  7  3  9]
 ..., 
 [ 6  9  6 ..., 12  2  4]
 [ 2  3  4 ...,  9  5  8]
 [ 4  9  6 ...,  7  2  8]]
[19  8  3 ..., 19 18  0]
Tuning hyper-parameters for accuracy


18, 0.738, 0.746, 3.018, 0.004, {'activation': 'relu', 'hidden_layer_sizes': 10, 'learning_rate': 'constant', 'learning_rate_init': 0.01, 'max_iter': 150, 'solver': 'sgd'}
15, 0.770, 0.783, 1.734, 0.004, {'activation': 'relu', 'hidden_layer_sizes': 10, 'learning_rate': 'constant', 'learning_rate_init': 0.01, 'max_iter': 150, 'solver': 'adam'}
17, 0.746, 0.753, 4.316, 0.004, {'activation': 'relu', 'hidden_layer_sizes': 10, 'learning_rate': 'constant', 'learning_rate_init': 0.01, 'max_iter': 200, 'solver': 'sgd'}
13, 0.771, 0.787, 1.898, 0.004, {'activation': 'relu', 'hidden_layer_sizes': 10, 'learning_rate': 'constant', 'learning_rate_init': 0.01, 'max_iter': 200, 'solver': 'adam'}
16, 0.751, 0.759, 5.315, 0.005, {'activation': 'relu', 'hidden_layer_sizes': 10, 'learning_rate': 'constant', 'learning_rate_init': 0.01, 'max_iter': 250, 'solver': 'sgd'}
14, 0.770, 0.783, 2.224, 0.005, {'activation': 'relu', 'hidden_layer_sizes': 10, 'learning_rate': 'constant', 'learning_rate_init': 0.01, 'max_iter': 250, 'solver': 'adam'}
12, 0.798, 0.808, 4.059, 0.005, {'activation': 'relu', 'hidden_layer_sizes': 25, 'learning_rate': 'constant', 'learning_rate_init': 0.01, 'max_iter': 150, 'solver': 'sgd'}
6, 0.864, 0.891, 2.582, 0.005, {'activation': 'relu', 'hidden_layer_sizes': 25, 'learning_rate': 'constant', 'learning_rate_init': 0.01, 'max_iter': 150, 'solver': 'adam'}
11, 0.810, 0.820, 5.450, 0.005, {'activation': 'relu', 'hidden_layer_sizes': 25, 'learning_rate': 'constant', 'learning_rate_init': 0.01, 'max_iter': 200, 'solver': 'sgd'}
4, 0.870, 0.901, 3.187, 0.005, {'activation': 'relu', 'hidden_layer_sizes': 25, 'learning_rate': 'constant', 'learning_rate_init': 0.01, 'max_iter': 200, 'solver': 'adam'}
9, 0.816, 0.832, 6.752, 0.005, {'activation': 'relu', 'hidden_layer_sizes': 25, 'learning_rate': 'constant', 'learning_rate_init': 0.01, 'max_iter': 250, 'solver': 'sgd'}
5, 0.864, 0.893, 2.919, 0.005, {'activation': 'relu', 'hidden_layer_sizes': 25, 'learning_rate': 'constant', 'learning_rate_init': 0.01, 'max_iter': 250, 'solver': 'adam'}
10, 0.812, 0.827, 4.941, 0.007, {'activation': 'relu', 'hidden_layer_sizes': 50, 'learning_rate': 'constant', 'learning_rate_init': 0.01, 'max_iter': 150, 'solver': 'sgd'}
1, 0.907, 0.958, 3.258, 0.007, {'activation': 'relu', 'hidden_layer_sizes': 50, 'learning_rate': 'constant', 'learning_rate_init': 0.01, 'max_iter': 150, 'solver': 'adam'}
8, 0.828, 0.846, 6.433, 0.007, {'activation': 'relu', 'hidden_layer_sizes': 50, 'learning_rate': 'constant', 'learning_rate_init': 0.01, 'max_iter': 200, 'solver': 'sgd'}
3, 0.904, 0.951, 2.774, 0.006, {'activation': 'relu', 'hidden_layer_sizes': 50, 'learning_rate': 'constant', 'learning_rate_init': 0.01, 'max_iter': 200, 'solver': 'adam'}
7, 0.844, 0.860, 7.991, 0.007, {'activation': 'relu', 'hidden_layer_sizes': 50, 'learning_rate': 'constant', 'learning_rate_init': 0.01, 'max_iter': 250, 'solver': 'sgd'}
2, 0.907, 0.951, 3.187, 0.007, {'activation': 'relu', 'hidden_layer_sizes': 50, 'learning_rate': 'constant', 'learning_rate_init': 0.01, 'max_iter': 250, 'solver': 'adam'}

Best parameters set found on development set LETTER
{'activation': 'relu', 'hidden_layer_sizes': 50, 'learning_rate': 'constant', 'learning_rate_init': 0.01, 'max_iter': 150, 'solver': 'adam'}

Best parameters classification report LETTER

             precision    recall  f1-score   support

          0       0.97      0.95      0.96       216
          1       0.92      0.91      0.91       239
          2       0.94      0.92      0.93       229
          3       0.91      0.93      0.92       248
          4       0.88      0.90      0.89       221
          5       0.91      0.89      0.90       246
          6       0.88      0.89      0.88       256
          7       0.78      0.90      0.84       202
          8       0.96      0.87      0.91       218
          9       0.93      0.91      0.92       233
         10       0.89      0.91      0.90       207
         11       0.94      0.96      0.95       231
         12       0.96      0.95      0.95       251
         13       0.94      0.95      0.95       223
         14       0.89      0.88      0.89       226
         15       0.99      0.89      0.94       249
         16       0.95      0.90      0.92       215
         17       0.90      0.84      0.87       234
         18       0.88      0.95      0.92       222
         19       0.92      0.94      0.93       237
         20       0.94      0.96      0.95       241
         21       0.96      0.91      0.93       247
         22       0.95      0.95      0.95       215
         23       0.93      0.96      0.94       245
         24       0.92      0.95      0.94       238
         25       0.92      0.95      0.94       211

avg / total       0.92      0.92      0.92      6000


Classification report
Parameters:
{'activation': 'relu', 'hidden_layer_sizes': 50, 'learning_rate': 'constant', 'learning_rate_init': 0.01, 'max_iter': 150, 'solver': 'adam'}
             precision    recall  f1-score   support

          0       0.96      0.97      0.97       216
          1       0.92      0.90      0.91       239
          2       0.95      0.91      0.93       229
          3       0.92      0.90      0.91       248
          4       0.92      0.90      0.91       221
          5       0.91      0.92      0.92       246
          6       0.88      0.86      0.87       256
          7       0.88      0.89      0.88       202
          8       0.94      0.94      0.94       218
          9       0.97      0.91      0.94       233
         10       0.84      0.95      0.89       207
         11       0.93      0.96      0.94       231
         12       0.98      0.96      0.97       251
         13       0.91      0.98      0.94       223
         14       0.89      0.91      0.90       226
         15       0.96      0.93      0.94       249
         16       0.95      0.93      0.94       215
         17       0.88      0.87      0.87       234
         18       0.89      0.98      0.93       222
         19       0.92      0.94      0.93       237
         20       0.97      0.94      0.96       241
         21       0.96      0.91      0.93       247
         22       0.91      0.98      0.95       215
         23       0.97      0.93      0.95       245
         24       0.97      0.93      0.95       238
         25       0.94      0.97      0.95       211

avg / total       0.93      0.93      0.93      6000


Confusion matrix, without normalization
[[210   0   0   3   0   0   0   0   0   1   0   0   0   0   1   0   0   0
    1   0   0   0   0   0   0   0]
 [  0 215   0   4   0   1   0   1   0   0   2   0   0   1   1   1   0   7
    5   0   0   0   0   0   1   0]
 [  0   0 216   0   3   0   1   0   0   0   1   1   0   0   3   0   0   0
    2   2   0   0   0   0   0   0]
 [  1   0   0 233   0   1   0   4   1   0   0   0   0   1   0   1   0   2
    1   2   1   0   0   0   0   0]
 [  0   1   0   0 196   1   2   0   0   0   2   9   0   0   0   0   1   1
    2   3   0   0   0   2   0   1]
 [  0   1   1   0   0 217   1   1   1   0   0   0   1   2   0   7   0   0
    3   8   0   2   0   1   0   0]
 [  0   0   3   3   3   0 227   0   0   0   3   1   1   0   3   1   0   8
    1   0   0   2   0   0   0   0]
 [  0   0   1   6   0   0   1 177   0   0   5   0   0   2   1   0   0   6
    0   0   1   0   0   2   0   0]
 [  0   0   1   1   0   1   0   0 202   7   0   1   0   0   0   2   0   0
    3   0   0   0   0   0   0   0]
 [  0   0   0   1   0   1   0   2   6 218   0   1   0   0   1   0   0   0
    3   0   0   0   0   0   0   0]
 [  0   0   0   1   1   0   0   6   0   0 187   2   0   0   0   0   0   2
    0   1   1   0   0   6   0   0]
 [  0   0   1   0   0   0   2   0   0   0   0 226   0   1   0   0   0   0
    1   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 246   1   0   1   0   0
    0   0   0   0   3   0   0   0]
 [  1   0   0   1   0   0   0   1   0   0   1   0   0 213   3   0   0   1
    0   0   0   0   0   0   2   0]
 [  2   0   2   2   0   1   1   1   0   1   0   0   0   0 209   0   3   2
    0   0   1   0   1   0   0   0]
 [  0   1   0   0   2   5   0   0   1   0   0   0   0   0   0 237   2   0
    0   0   0   0   0   0   1   0]
 [  3   2   0   0   0   0   2   0   0   0   0   2   0   0   0   1 202   0
    0   0   0   0   0   0   0   3]
 [  0   8   0   1   1   0   0   5   0   1   3   1   0   2   0   1   0 210
    0   1   0   0   0   0   0   0]
 [  0   1   0   0   0   1   0   1   0   0   0   1   0   0   0   0   0   0
  217   0   0   0   0   1   0   0]
 [  0   0   0   1   0   1   0   0   0   0   1   1   0   0   0   1   0   2
    0 225   1   1   0   0   3   0]
 [  0   0   1   0   0   0   0   4   0   0   1   0   2   0   1   0   0   0
    0   0 230   0   2   0   0   0]
 [  1   2   0   0   0   1   0   2   0   1   0   0   1   1   0   0   1   2
    0   2   0 230   1   0   2   0]
 [  1   1   0   0   0   0   0   1   0   0   0   0   3   0   0   0   0   0
    0   0   0   1 208   0   0   0]
 [  0   0   0   1   2   1   0   0   1   0   2   0   0   0   1   0   0   1
    2   1   1   0   0 231   1   0]
 [  4   0   0   0   0   0   0   0   0   0   0   0   0   0   0   3   1   0
    5   5   1   3   0   3 213   0]
 [  1   0   0   0   5   0   0   0   0   1   0   1   0   0   0   0   0   0
    7   1   0   0   0   0   0 195]]
